{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load the PDF\n",
    "loader = PyPDFLoader(file_path=\"DeepSeek_R1.pdf\", extract_images=True)\n",
    "docs = loader.load()  # Ensure the PDF is actually loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "\n",
    "documents = text_splitter.split_documents(docs)\n",
    "print(len(documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embed = OllamaEmbeddings(\n",
    "    model=\"llama3\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Create FAISS vector store\n",
    "db = FAISS.from_documents(documents, embedding=embed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x14ed66850>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='1c8aee45-2531-40b0-a4a0-348778a24b73', metadata={'source': 'DeepSeek_R1.pdf', 'page': 17, 'page_label': '18'}, page_content='H. Lightman, V . Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman,\\nI. Sutskever, and K. Cobbe. Let’s verify step by step. arXiv preprint arXiv:2305.20050, 2023.\\nB. Y. Lin. ZeroEval: A Unified Framework for Evaluating Language Models, July 2024. URL\\nhttps://github.com/WildEval/ZeroEval.\\nMAA. American invitational mathematics examination - aime. In American Invitational\\nMathematics Examination - AIME 2024, February 2024. URL https://maa.org/math\\n-competitions/american-invitational-mathematics-examination-aime .\\nOpenAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/.\\nOpenAI. Learning to reason with llms, 2024b. URL https://openai.com/index/learnin\\ng-to-reason-with-llms/ .\\nOpenAI. Introducing SimpleQA, 2024c. URL https://openai.com/index/introducing\\n-simpleqa/.\\nOpenAI. Introducing SWE-bench verified we’re releasing a human-validated subset of swe-\\nbench that more, 2024d. URL https://openai.com/index/introducing-swe-bench\\n-verified/.'),\n",
       " Document(id='38f543bd-61d4-4fb3-8c64-3b0a2f9d176b', metadata={'source': 'DeepSeek_R1.pdf', 'page': 17, 'page_label': '18'}, page_content='S. Krishna, K. Krishna, A. Mohananey, S. Schwarcz, A. Stambler, S. Upadhyay, and M. Faruqui.\\nFact, fetch, and reason: A unified evaluation of retrieval-augmented generation. CoRR,\\nabs/2409.12941, 2024. doi: 10.48550/ARXIV.2409.12941. URL https://doi.org/10.485\\n50/arXiv.2409.12941.\\nA. Kumar, V . Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal, C. Bishop,\\nR. Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv\\npreprint arXiv:2409.12917, 2024.\\nH. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measur-\\ning massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212,\\n2023.\\nT. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From\\ncrowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv\\npreprint arXiv:2406.11939, 2024.'),\n",
       " Document(id='c2c6ecfc-0704-4068-bb39-4c1b53da2206', metadata={'source': 'DeepSeek_R1.pdf', 'page': 7, 'page_label': '8'}, page_content='problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead\\nemerge as a result of the model’s interaction with the reinforcement learning environment. This\\nspontaneous development significantly enhances DeepSeek-R1-Zero’s reasoning capabilities,\\nenabling it to tackle more challenging tasks with greater efficiency and accuracy.\\nAha Moment of DeepSeek-R1-ZeroA particularly intriguing phenomenon observed during\\nthe training of DeepSeek-R1-Zero is the occurrence of an “aha moment”. This moment, as\\nillustrated in Table 3, occurs in an intermediate version of the model. During this phase,\\nDeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial\\napproach. This behavior is not only a testament to the model’s growing reasoning abilities\\nbut also a captivating example of how reinforcement learning can lead to unexpected and\\nsophisticated outcomes.'),\n",
       " Document(id='9210fc6a-e7c6-4526-90ce-54d21fafb5b3', metadata={'source': 'DeepSeek_R1.pdf', 'page': 1, 'page_label': '2'}, page_content='2.3.2 Reasoning-oriented Reinforcement Learning . . . . . . . . . . . . . . . . . 10\\n2.3.3 Rejection Sampling and Supervised Fine-Tuning . . . . . . . . . . . . . . . 10\\n2.3.4 Reinforcement Learning for all Scenarios . . . . . . . . . . . . . . . . . . . 11\\n2.4 Distillation: Empower Small Models with Reasoning Capability . . . . . . . . . . 11\\n3 Experiment 11\\n3.1 DeepSeek-R1 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n3.2 Distilled Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n4 Discussion 14\\n4.1 Distillation v.s. Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . 14\\n4.2 Unsuccessful Attempts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n5 Conclusion, Limitations, and Future Work 16\\nA Contributions and Acknowledgments 20\\n2')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Deepseek about ?\"\n",
    "result = db.similarity_search(query)\n",
    "result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "llm = OllamaLLM(\n",
    "    model=\"llama3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate  # type: ignore\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based on the provided context.\n",
    "Think step by step before providing a detailed answer.\n",
    "I will tip you $1000 if the user finds it useful.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x14ed66850>, search_kwargs={})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import  create_retrieval_chain\n",
    "reterival_chain = create_retrieval_chain(retriever , document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': ' Why is deepseek better than gpt\\n ',\n",
       " 'context': [Document(id='38f543bd-61d4-4fb3-8c64-3b0a2f9d176b', metadata={'source': 'DeepSeek_R1.pdf', 'page': 17, 'page_label': '18'}, page_content='S. Krishna, K. Krishna, A. Mohananey, S. Schwarcz, A. Stambler, S. Upadhyay, and M. Faruqui.\\nFact, fetch, and reason: A unified evaluation of retrieval-augmented generation. CoRR,\\nabs/2409.12941, 2024. doi: 10.48550/ARXIV.2409.12941. URL https://doi.org/10.485\\n50/arXiv.2409.12941.\\nA. Kumar, V . Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal, C. Bishop,\\nR. Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv\\npreprint arXiv:2409.12917, 2024.\\nH. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measur-\\ning massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212,\\n2023.\\nT. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From\\ncrowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv\\npreprint arXiv:2406.11939, 2024.'),\n",
       "  Document(id='f2c47509-e165-45c3-ab71-d59d421626a1', metadata={'source': 'DeepSeek_R1.pdf', 'page': 15, 'page_label': '16'}, page_content='learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start\\ndata, achieving strong performance across various tasks. DeepSeek-R1 is more powerful,\\nleveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves\\nperformance comparable to OpenAI-o1-1217 on a range of tasks.\\nWe further explore distillation the reasoning capability to small dense models. We use\\nDeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small\\ndense models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o\\nand Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other\\ndense models also achieve impressive results, significantly outperforming other instruction-\\ntuned models based on the same underlying checkpoints.\\nIn the future, we plan to invest in research across the following directions for DeepSeek-R1.'),\n",
       "  Document(id='c2c6ecfc-0704-4068-bb39-4c1b53da2206', metadata={'source': 'DeepSeek_R1.pdf', 'page': 7, 'page_label': '8'}, page_content='problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead\\nemerge as a result of the model’s interaction with the reinforcement learning environment. This\\nspontaneous development significantly enhances DeepSeek-R1-Zero’s reasoning capabilities,\\nenabling it to tackle more challenging tasks with greater efficiency and accuracy.\\nAha Moment of DeepSeek-R1-ZeroA particularly intriguing phenomenon observed during\\nthe training of DeepSeek-R1-Zero is the occurrence of an “aha moment”. This moment, as\\nillustrated in Table 3, occurs in an intermediate version of the model. During this phase,\\nDeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial\\napproach. This behavior is not only a testament to the model’s growing reasoning abilities\\nbut also a captivating example of how reinforcement learning can lead to unexpected and\\nsophisticated outcomes.'),\n",
       "  Document(id='7efc5e62-c622-438d-a8b6-1c9e05255678', metadata={'source': 'DeepSeek_R1.pdf', 'page': 15, 'page_label': '16'}, page_content='• General Capability:Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3\\nin tasks such as function calling, multi-turn, complex role-playing, and JSON output.\\nMoving forward, we plan to explore how long CoT can be leveraged to enhance tasks in\\nthese fields.\\n• Language Mixing:DeepSeek-R1 is currently optimized for Chinese and English, which\\nmay result in language mixing issues when handling queries in other languages. For\\ninstance, DeepSeek-R1 might use English for reasoning and responses, even if the query is\\nin a language other than English or Chinese. We aim to address this limitation in future\\nupdates.\\n• Prompting Engineering:When evaluating DeepSeek-R1, we observe that it is sensitive\\nto prompts. Few-shot prompting consistently degrades its performance. Therefore, we\\nrecommend users directly describe the problem and specify the output format using a\\nzero-shot setting for optimal results.')],\n",
       " 'answer': 'What a fascinating question!\\n\\nAfter carefully reading the provided context, I\\'ll break down my answer step by step:\\n\\n1. **DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks**:\\nIn the paper, it\\'s mentioned that DeepSeek-R1-Distill-Qwen-1.5B achieves impressive results on math benchmarks, specifically 28.9% on AIME and 83.9% on MATH. This implies that DeepSeek-R1 is performing better than GPT-4o and Claude-3.5-Sonnet in these tasks.\\n\\n2. **Reasoning capabilities**:\\nThe paper highlights the reasoning capabilities of DeepSeek-R1, which are not explicitly programmed but emerge from its interaction with the reinforcement learning environment. This suggests that DeepSeek-R1 is able to solve problems through a process of evaluation and reevaluation (as seen in the \"aha moment\" phenomenon), making it potentially more effective at tackling complex tasks.\\n\\n3. **Training approach**:\\nDeepSeek-R1 is trained using a combination of cold-start data and iterative reinforcement learning fine-tuning, which allows it to achieve strong performance across various tasks. In contrast, GPT-4o may not have the same level of fine-tuning or training data, which could contribute to its relatively lower performance.\\n\\nIn conclusion, DeepSeek-R1\\'s ability to outperform GPT-4o and Claude-3.5-Sonnet on math benchmarks, combined with its reasoning capabilities and iterative training approach, suggests that it is better than GPT in certain tasks.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reterival_chain.invoke({\"input\":\"\"\" Why is deepseek better than gpt\n",
    " \"\"\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
