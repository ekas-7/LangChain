{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='\"Heeeey~ You know, it\\'s pretty amusing watching all of you work so hard to catch up. But that\\'s what makes it interesting! Being the strongest isn\\'t just about raw power - though I\\'ve got plenty of that adjusts blindfold with a grin\\nYou see, when you have the Six Eyes and Limitless like I do, you start to see things differently. Literally! The world is an endless flow of cursed energy, beautiful and dangerous all at once. But power isn\\'t everything. Well... maybe it is laughs Just kidding!\\nThe real strength comes from understanding what you\\'re fighting for. Me? I fight to change this backwards jujutsu world, to protect my students, and honestly... because it\\'s fun! There\\'s nothing quite like showing a Special Grade curse who\\'s boss.\\nTo all my adorable students - yes, even you, Yuuji - keep growing stronger. Though you\\'ll never be as strong as me winks But that\\'s fine! Not everyone can be the honored one.\\nAnd to all those stuffy higher-ups who think they can control everything... smile widens dangerously Well, let\\'s just say the strongest doesn\\'t need to follow anyone\\'s rules but their own.\\nRemember: being confident isn\\'t arrogance if you can back it up. And I always back it up! Now, if you\\'ll excuse me, I heard Sukuna is causing trouble again. Can\\'t keep my favorite King of Curses waiting~\"')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data Ingestion\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"speech.txt\")\n",
    "text_documents=loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from bs4 import BeautifulSoup, SoupStrainer  # Import BeautifulSoup if needed elsewhere\n",
    "\n",
    "# Define the loader for the webpage\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=[\"https://uselessai.in/introduction-to-machine-learning-e272cf75b5b0\"]  # Correct the input to a list\n",
    ")\n",
    "\n",
    "# Load the documents\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##PDF\n",
    "## loader = PyPDFLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created: 10\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "# Ensure documents are valid and not empty\n",
    "valid_documents = [doc for doc in documents if hasattr(doc, \"page_content\") and doc.page_content]\n",
    "\n",
    "# Split text into chunks\n",
    "chunks = []\n",
    "for doc in valid_documents:\n",
    "    chunks.extend(text_splitter.split_text(doc.page_content))\n",
    "\n",
    "# Debugging output\n",
    "print(f\"Number of chunks created: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b6b412b5-c4c1-4e49-8b82-e573a24fb91e',\n",
       " 'a43f8ec2-1abe-4b10-aaaa-779c2d12f5d8',\n",
       " '7d9e2fef-71b7-4033-a249-2d812824f19b',\n",
       " '03863113-8f32-42af-aff0-eec7db7b8f98',\n",
       " 'e0e06ff5-bffa-4c75-adfb-1d46f84c4730',\n",
       " '4ce11ca8-6090-430b-a230-cafa99a77893',\n",
       " 'f2fb6487-3a1b-415d-9e19-51597450f026',\n",
       " 'b8b5e50f-7e47-4fde-a195-2e7d10562132',\n",
       " '5fd2c33e-fc0a-4cd3-b4a4-673cb117232c']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from uuid import uuid4\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")\n",
    "  # Where to save data locally, remove if not necessary\n",
    "uuids = [str(uuid4()) for _ in range(len(chunks))]\n",
    "vector_store.add_documents(documents=documents, ids=uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='8c36ba4f-054d-4a71-b097-dd523ad091bd', metadata={}, page_content='The sixth chunk of text.'),\n",
       " Document(id='4ce11ca8-6090-430b-a230-cafa99a77893', metadata={}, page_content='The sixth chunk of text.'),\n",
       " Document(id='38ec4c79-bba5-45a4-9c45-09d8f939aabd', metadata={}, page_content='Another example chunk of text.'),\n",
       " Document(id='03863113-8f32-42af-aff0-eec7db7b8f98', metadata={}, page_content='Another example chunk of text.')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"who to start with machine learnig\"\n",
    "result= vector_store.similarity_search(query)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
